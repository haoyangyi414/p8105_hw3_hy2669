p8105\_hw3\_hy2669
================
haoyang,yi
2020/10/9

## Problem 1

``` r
data("instacart")
```

The dataset contains 1384617 rows and 15 columns. Observations are level
of items in orders by user, information of items.

``` r
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

``` r
instacart %>% 
  count(aisle) %>% 
  filter(n>10000) %>%
  mutate(aisle = factor(aisle),
         aisle = fct_reorder(aisle,n)) %>% 
  ggplot(aes(x = aisle,y = n)) +
  geom_point()+
  theme(axis.text.x = element_text(angle=90,vjust=0.5,hjust=1))
```

<img src="p8105_hw3_hy2669_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

``` r
instacart %>%
  filter(aisle %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle,rank) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow,
              values_from = mean_hour)
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

## Problem 2

``` r
accel_df=read_csv('./data/accel_data.csv') %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = starts_with('activity'),
    names_to = 'minute',
    values_to = 'activity',
    names_prefix = 'activity_') %>% # represent the count of activity 1-1440 variables of each minute in column
  mutate(weekday_vs_weekend = day %in% c('Saturday','Sunday'),
           weekday_vs_weekend = case_when(weekday_vs_weekend ~ 'weekend',
                                          !weekday_vs_weekend ~ 'weekday')) %>% # create new variable weekday_vs_weekend
  mutate(weekday_vs_weekend = as.factor(weekday_vs_weekend),
         minute = as.numeric(minute),
         week = as.factor(week),
         week = forcats::fct_relevel(week),
         day = as.factor(day),
         day = forcats::fct_relevel(day,c("Monday","Tuesday","Wednesday","Thursday",
          "Friday", "Saturday","Sunday"))) %>%  # convert variable classes to be reasonable.
  group_by(week) %>%
  arrange(day,.by_group = TRUE)  # rearrange day variable so that is starts from Monday to Sunday.
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
  head(accel_df)
```

    ## # A tibble: 6 x 6
    ## # Groups:   week [1]
    ##   week  day_id day    minute activity weekday_vs_weekend
    ##   <fct>  <dbl> <fct>   <dbl>    <dbl> <fct>             
    ## 1 1          2 Monday      1        1 weekday           
    ## 2 1          2 Monday      2        1 weekday           
    ## 3 1          2 Monday      3        1 weekday           
    ## 4 1          2 Monday      4        1 weekday           
    ## 5 1          2 Monday      5        1 weekday           
    ## 6 1          2 Monday      6        1 weekday

``` r
  tail(accel_df)
```

    ## # A tibble: 6 x 6
    ## # Groups:   week [1]
    ##   week  day_id day    minute activity weekday_vs_weekend
    ##   <fct>  <dbl> <fct>   <dbl>    <dbl> <fct>             
    ## 1 5         32 Sunday   1435        1 weekend           
    ## 2 5         32 Sunday   1436        1 weekend           
    ## 3 5         32 Sunday   1437        1 weekend           
    ## 4 5         32 Sunday   1438        1 weekend           
    ## 5 5         32 Sunday   1439        1 weekend           
    ## 6 5         32 Sunday   1440        1 weekend

The resulting dataset has 6 variables which are week(1 to 5), day\_id (1
to 35), day(Monday,Tuesday..Sunday), minute in each day(1 to 1440),
activity and weekday\_vs\_weekend. There are 50400 observations of these
variables. Week,day,weekday\_vs\_weekend are factor variables, day\_id,
minute, activity are numeric factors.

``` r
accel_df %>% 
  group_by(week,day) %>% 
  summarize(total_activity_day = sum(activity)) %>% 
  mutate(total_activity_day = round(total_activity_day)) %>%
  pivot_wider(names_from = week,
              names_prefix = 'week_',
              values_from = total_activity_day) %>% 
  knitr::kable()
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

| day       | week\_1 | week\_2 | week\_3 | week\_4 | week\_5 |
| :-------- | ------: | ------: | ------: | ------: | ------: |
| Monday    |   78828 |  295431 |  685910 |  409450 |  389080 |
| Tuesday   |  307094 |  423245 |  381507 |  319568 |  367824 |
| Wednesday |  340115 |  440962 |  468869 |  434460 |  445366 |
| Thursday  |  355924 |  474048 |  371230 |  340291 |  549658 |
| Friday    |  480543 |  568839 |  467420 |  154049 |  620860 |
| Saturday  |  376254 |  607175 |  382928 |    1440 |    1440 |
| Sunday    |  631105 |  422018 |  467052 |  260617 |  138421 |

The table containing total daily activities on each day of weeks are
created. The table shows that total daily activities on Saturday and
Sunday decrease rapidly in week 4, other days in week 4 also show
decrease in total activities. Among 7 days of a week,
Monday,Friday,Saturday and Sunday have apparent trends from week 1 to 5,
Tuesday and Wednesday have relatively steady trends.

``` r
accel_df %>%
  group_by(day_id) %>% 
  mutate(minute_day = 1,
         hour_oneday = cumsum(minute_day)%/%60) %>%
  select(-minute_day) %>%
  group_by(day_id,hour_oneday) %>%
  mutate(activity_hour = sum(activity)) %>% 
  ggplot(aes(x = hour_oneday,y = activity_hour, colour = day, group = day_id))+
  stat_smooth(alpha = 0.7, se = F,geom = 'line',method = 'gam')+
  scale_x_continuous(name = 'Hour', limits = c(0,24),breaks = seq(0,24,3))+
  scale_y_continuous(name = 'Activity',trans = 'sqrt',limits = c(0,70000),
                     breaks = c(0,2000,10000,40000))+ # modify the scale of y to make the curve more concentrated
  labs(title = 'Smooth line of 24-hour activity for 35 days')+
  theme(legend.position = 'bottom')+guides(color = guide_legend(nrow =1)) # put legend at the bottom and in a row.
```

    ## `geom_smooth()` using formula 'y ~ s(x, bs = "cs")'

    ## Warning: Removed 600 rows containing non-finite values (stat_smooth).

    ## Warning: Removed 33 row(s) containing missing values (geom_path).

<img src="p8105_hw3_hy2669_files/figure-gfm/make the plot-1.png" width="90%" />

In this graph, most of smoothed lines overlap at 0-6 am and 9-12 pm
which indicates that the total hourly activity in this period may follow
a daily routine: increase in 0-6 am and decrease in 9-12 pm. Hourly
activity reach the peak in daytime, around 8-9 am and 4-7 pm. In 1-3 pm
there is a fluctuation, most of lines decrease and recover after 3 pm.
Sunday, Monday and Friday have relatively higher and steady peaks.

## Problem 3

``` r
data("ny_noaa")
noaa_df = ny_noaa %>%
    separate(date,into=c("year","month","day"),sep="-",remove = F) %>% # separate date variable into year, month and day.
    mutate(year = as.numeric(year), month = as.numeric(month),day = as.numeric(day),
           tmin = as.numeric(tmin), tmax = as.numeric(tmax),
           tmin = tmin/10, tmax = tmax/10,
           prcp = prcp/10,
           snow = as.integer(snow)) # Convert the type of variables, change the unit of temperature from tenths of degree C into degree C. Change the unit of precipitation from tenths of mm into mm. 
skimr::skim_without_charts(noaa_df)
```

|                                                  |          |
| :----------------------------------------------- | :------- |
| Name                                             | noaa\_df |
| Number of rows                                   | 2595176  |
| Number of columns                                | 10       |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |          |
| Column type frequency:                           |          |
| character                                        | 1        |
| Date                                             | 1        |
| numeric                                          | 8        |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |          |
| Group variables                                  | None     |

Data summary

**Variable type: character**

| skim\_variable | n\_missing | complete\_rate | min | max | empty | n\_unique | whitespace |
| :------------- | ---------: | -------------: | --: | --: | ----: | --------: | ---------: |
| id             |          0 |              1 |  11 |  11 |     0 |       747 |          0 |

**Variable type: Date**

| skim\_variable | n\_missing | complete\_rate | min        | max        | median     | n\_unique |
| :------------- | ---------: | -------------: | :--------- | :--------- | :--------- | --------: |
| date           |          0 |              1 | 1981-01-01 | 2010-12-31 | 1997-01-21 |     10957 |

**Variable type: numeric**

| skim\_variable | n\_missing | complete\_rate |    mean |     sd |     p0 |    p25 |    p50 |    p75 |  p100 |
| :------------- | ---------: | -------------: | ------: | -----: | -----: | -----: | -----: | -----: | ----: |
| year           |          0 |           1.00 | 1996.50 |   9.19 | 1981.0 | 1988.0 | 1997.0 | 2005.0 |  2010 |
| month          |          0 |           1.00 |    6.56 |   3.45 |    1.0 |    4.0 |    7.0 |   10.0 |    12 |
| day            |          0 |           1.00 |   15.73 |   8.80 |    1.0 |    8.0 |   16.0 |   23.0 |    31 |
| prcp           |     145838 |           0.94 |    2.98 |   7.82 |    0.0 |    0.0 |    0.0 |    2.3 |  2286 |
| snow           |     381221 |           0.85 |    4.99 |  27.22 | \-13.0 |    0.0 |    0.0 |    0.0 | 10160 |
| snwd           |     591786 |           0.77 |   37.31 | 113.54 |    0.0 |    0.0 |    0.0 |    0.0 |  9195 |
| tmax           |    1134358 |           0.56 |   13.98 |  11.14 | \-38.9 |    5.0 |   15.0 |   23.3 |    60 |
| tmin           |    1134420 |           0.56 |    3.03 |  10.40 | \-59.4 |  \-3.9 |    3.3 |   11.1 |    60 |

``` r
  most_common<-function(x){
    uniquevalue<-unique(x)
    uniquevalue[which.max(tabulate(match(x,uniquevalue)))]
  } # create a funcition to find the most common value of snow
```

The dataset contains data from 1981-01-01 to 2010-12-31,of 747 stations.
It contains 10 variables and 2595176 observations. The key variables are
id-id of weather stations, date-date of observations,
prcp-precipitation(mm), snow-snowfall(mm), snwd-snow depth(mm), tmin and
tmax-minimum and maximum temperature(degrees C) of the day. Missing data
is a significant issue in temperature since the complete rate of
tmax,tmin variables is only 0.563,which indicates that almost a half of
data will be lost if we choose to remove NA. Also, the missing of
snowfall and snow depth values is considerable. For snowfall, the most
commonly observed values is 0mm. Because NY only have snow at Winter,
for the rest of a year there is no snow in NY, which means snowfall is 0
mm.

``` r
plot_jan = noaa_df %>%
  filter(month %in% c(1)) %>%
  group_by(id,year,month) %>% # group by distinct stations, years and months
  summarize(avg_tmax = mean(tmax,na.rm = TRUE)) %>% # create average of maximum of temperature.
  ggplot(aes(x = year,y = avg_tmax,color = id, group = id))+
  geom_path(alpha=0.6)+geom_point(alpha=0.6)+ 
  scale_x_continuous(limits = c(1981,2010),breaks = seq(1981,2010,2))+
  theme(legend.position = "none")+ # do not need to show legends, legends also cause the figure become too large to display.
  labs(title = 'Average max temperature in January from 1981 to 2010')+
  xlab('Year')+ylab('Average max temperature in January')
```

    ## `summarise()` regrouping output by 'id', 'year' (override with `.groups` argument)

``` r
plot_Jul = noaa_df %>%
  filter(month %in% c(7)) %>% 
  group_by(id,year,month) %>% 
  summarize(avg_tmax = mean(tmax,na.rm = TRUE)) %>%
  ggplot(aes(x = year,y = avg_tmax,color = id, group = id))+
  geom_path(alpha=0.5)+geom_point(alpha=0.2)+
  scale_x_continuous(limits = c(1981,2010),breaks = seq(1981,2010,2))+
  theme(legend.position = "none")+ 
  labs(title = 'Average highest temperature in July from 1981 to 2010')+
  xlab('Year')+ylab('Average highest temperature in July')
```

    ## `summarise()` regrouping output by 'id', 'year' (override with `.groups` argument)

``` r
plot_jan/plot_Jul
```

<img src="p8105_hw3_hy2669_files/figure-gfm/plot of avg tmax-1.png" width="90%" />

For January, average highest temperatures from 1981 to 2010 mostly fall
in -7.5 to 5 degree C. Usually the average highest temperatures do not
exceed the range from -8 to 8 degree C .Outliers appear in
1998,1999,2004 when one or several station observed exceptionally higher
average of highest temperatures than other stations, and in
1982,1984,1988,1989,1990,1996,2004,2007 when one or several stations
observed exceptionally lower average of highest temperatures than other
stations. For July, average highest temperatures from 1981 to 2010
mostly fall in 25 to 30 degree C. Usually the average highest
temperatures do not exceed the range from 22.5 to 31 degree C. Outliers
appear in 1981,1983,1984,1988,1989,1990,1996,2004,2007 when one or
several stations observed exceptionally lower average of highest
temperatures than other stations. Compared with January, the trend of
July is more steady(less fluctuate), with no significant change among
these 40 years.

``` r
plot_vs = noaa_df %>%
  ggplot(aes(x = tmin, y= tmax))+
  geom_hex(bins = 50,col = "grey",alpha=0.9)+ #create a hexplot to show tmin vs tmax
  scale_x_continuous(breaks = seq(-60,60,10),limits = c(-60,60))+
  scale_y_continuous(breaks = seq(-40,60,10))+
  labs(title = "Limits of temperature in 1981-2010")+
  xlab("Lowest temperature (C)")+ ylab("Highest temperature (C)")
plot_dist = noaa_df %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = snow),group = as.factor(year))+ 
  geom_density_ridges(aes(y = year,group = as.factor(year)),alpha = 0.8)+ 
  # use density_ridges to show distributions of snow value of each year.
  scale_x_continuous(limits = c(0,100),breaks = seq(0,100,20))+
  scale_y_continuous(breaks = seq(1981,2010,2))+
  labs(title = "Snowfall >0 & <100 in 40 years")+
  xlab('Snowfall value (mm)')+ ylab("Year")
plot_vs+plot_dist  
```

    ## Picking joint bandwidth of 3.76

<img src="p8105_hw3_hy2669_files/figure-gfm/tmax vs tmin and dist of snow-1.png" width="90%" />

The hexplot shows that the lowest and the highest temperature of one day
nearly have a positive correlation. Top common combinations of the
lowest and highest temperatures are those light green and yellow
hexes,approximately located at (0,5) (0,8) (10,25) (15,27). The density
ridge shows that the distribution of snowfall value \>0 and \<100
changed slightly from 1981 to 2010, the major change is the peak of
distribution. From 1981 to 1999, the peak is located at around 25 mm.
From 2001 to 2010, the peak starts to move to a lower snowfall value and
it’s located at around 9 mm in 2010. Also, the density of snowfall \>20
mm becomes flatten in 2008-2010. This change indicates the trend of
global warming and decrease of snowfall during that period.
